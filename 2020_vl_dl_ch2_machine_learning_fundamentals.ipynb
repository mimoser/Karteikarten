{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "2020-vl-dl-ch2-machine-learning-fundamentals.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mimoser/karteikarten/blob/master/2020_vl_dl_ch2_machine_learning_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYXJRWI-nj-c",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 2: Machine Learning Fundamentals\n",
        "\n",
        "In this chapter, we will look at some of the fundamental concepts in Machine Learning. To illustrate them, we are going to use a dataset about cars from the 1970s. A preprocessed version of the dataset is available in Moodle. You can find more details about the [original dataset](https://archive.ics.uci.edu/ml/datasets/Auto+MPG) at the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8T8Wzjtnj-d",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Supervised vs. Unsupervised Learning\n",
        "\n",
        "Nothing to show here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fueMwh-nj-d",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Feature Types and Encoding\n",
        "\n",
        "As a first step, we import the dataset, which is stored as a CSV file, using Pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9-80kmcnj-e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "86b65a76-6917-4478-ca43-e63312222d3b"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "cars = pd.read_csv('../data/auto-mpg.data', header=None, sep='\\s+', names=['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'origin', 'car name'])\n",
        "cars"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0b608238750c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/auto-mpg.data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\s+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cylinders'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'displacement'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'horsepower'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acceleration'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'origin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'car name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ../data/auto-mpg.data does not exist: '../data/auto-mpg.data'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3s0hNS5nj-h",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Linear Regression\n",
        "\n",
        "In the first step, we want to use simple linear regression to predict fuel efficiency (mpg) based on the horsepowers of a car. First, we need to extract the relevant features and plot them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNv4rrrqnj-h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "cd0a93d6-d524-4319-a1f1-945c523bbf3c"
      },
      "source": [
        "# extract horsepower and mpg\n",
        "hp, mpg = cars['horsepower'].values, cars['mpg'].values\n",
        "\n",
        "# plot hp and mpg\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(hp, mpg)\n",
        "plt.xlabel('HP')\n",
        "plt.ylabel('MPG')\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bfc92a23b70b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# extract horsepower and mpg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmpg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'horsepower'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mpg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# plot hp and mpg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cars' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfAnVKuUnj-k",
        "colab_type": "text"
      },
      "source": [
        "Before training our simple linear regression model, it is a good idea to normalize our data. Here, the normalization is done on the NumPy arrays that have been extracted from the DataFrame provided by Pandas. Later on, we will see how we can normalized directly in Pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSrWv9Rynj-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize horsepower and mpg\n",
        "hp_min, hp_max = min(hp), max(hp)\n",
        "hp_n = (hp - hp_min) / (hp_max - hp_min)\n",
        "mpg_min, mpg_max = min(mpg), max(mpg)\n",
        "mpg_n = (mpg - mpg_min) / (mpg_max - mpg_min)\n",
        "\n",
        "# plot normalized hp and mpg\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(hp_n, mpg_n)\n",
        "plt.xlabel('HP')\n",
        "plt.ylabel('MPG')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGW6IGYAnj-n",
        "colab_type": "text"
      },
      "source": [
        "Note that the plot looks the same, only the ranges of the two axes have changed and are now both $[0, 1]$.\n",
        "\n",
        "Next, we can learn a simple regression model using Keras. The details of how the model is specified will become clearer, once we look into neural networks. In this model, we use a single neuron (i.e., linear combination of inputs) with a linear activation function and mean squared error as our loss function -- this corresponds exactly to simple linear regression. The parameters are trained using Stochastic Gradient Descent, which is run for 30 iterations using samples of size one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMI7zvA_nj-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# set up model with a single neuron using a linear activation function and MSE as loss function\n",
        "model = Sequential()\n",
        "model.add(Dense(1, activation='linear', input_dim=1))\n",
        "model.compile(optimizer='SGD', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "model.fit(hp_n, mpg_n, epochs=30, batch_size=1, verbose=0)\n",
        "\n",
        "# determine loss, mean squared error, and mean absolute error\n",
        "loss_n, mse_n = model.evaluate(hp_n, mpg_n, verbose=1)\n",
        "\n",
        "print('Mean squared error (MSE): %f' % mse_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5caTD9jpnj-q",
        "colab_type": "text"
      },
      "source": [
        "These error values are in terms of normalized mpg. In order to make them more interpretable, we have to undo the normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XmSjhtqnj-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "mse = mse_n * (mpg_max - mpg_min) + mpg_min\n",
        "\n",
        "print('Mean squared error (MSE): %f' % mse)\n",
        "print('Root mean squared error (MAE): %f' % math.sqrt(mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcvasGCOnj-t",
        "colab_type": "text"
      },
      "source": [
        "This means that on average we are off by about 3 miles when predicting mpg. Finally, we can plot the data points together with the regression line that we just determined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbS4Y_renj-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot hp and mpg\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(hp, mpg)\n",
        "plt.xlabel('HP')\n",
        "plt.ylabel('MPG')\n",
        "\n",
        "# plot the regression line: undo normalization for predictions at 0.0 and 1.0\n",
        "plt.plot([hp_min, hp_max], model.predict([0.0, 1.0]) * (mpg_max - mpg_min) + mpg_min, color='red')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxzCDxAqnj-w",
        "colab_type": "text"
      },
      "source": [
        "We can also make use of multiple input features to predict our target feature (mpg). For the following example, we are going to use horsepower, weight, and acceleration to predict mpg. Again, we need to normalize the different features, but this time we will achieve this using functionality from Pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBtvU7Sonj-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract relevant features from dataset and normalize them\n",
        "cars_hwam = cars[['horsepower', 'weight', 'acceleration', 'mpg']]\n",
        "cars_n = (cars_hwam - cars_hwam.min()) / (cars_hwam.max() - cars_hwam.min())\n",
        "\n",
        "X = cars_n[['horsepower', 'weight', 'acceleration']].values\n",
        "y = cars_n['mpg']\n",
        "\n",
        "# set up model with a single neuron using a linear activation function and MSE as loss function\n",
        "model = Sequential()\n",
        "model.add(Dense(1, activation='linear', input_dim=X.shape[1]))\n",
        "model.compile(optimizer='SGD', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "model.fit(X, y, epochs=30, batch_size=1, verbose=0)\n",
        "\n",
        "# determine loss, mean squared error, and mean absolute error\n",
        "loss_n, mse_n = model.evaluate(X, y, verbose=0)\n",
        "\n",
        "print('Mean squared error (MSE): %f' % mse_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4mdUs9_nj-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mse = mse_n * (mpg_max - mpg_min) + mpg_min\n",
        "\n",
        "print('Mean squared error (MSE): %f' % mse)\n",
        "print('Root mean squared error (MAE): %f' % math.sqrt(mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYGz54kdnj-4",
        "colab_type": "text"
      },
      "source": [
        "As we can see, adding the two additional features did not give us a signficant improvement in RMSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-tCMNH5nj-5",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 Logistic Regression\n",
        "\n",
        "In this first example, we want to determine the origin of cars using Logistic Regression. More precisely, we only want to distinguish between cars from the U.S. and cars that are not from the U.S. -- remember that Logistic Regression is a binary classification method. As input features we will use horsepower and weight and determine suitable binary values for the origin feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggWBrVmanj-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# determine (n x 2) data matrix from normalized horsepower values\n",
        "X = cars_n[['horsepower', 'weight']].values\n",
        "\n",
        "# replace origin for Japanese and European cars by 0\n",
        "y = cars['origin'].replace([1,2,3], [1,0,0]).values\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1, activation='sigmoid', input_dim=X.shape[1]))\n",
        "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=30, batch_size=1, verbose=0)\n",
        "loss, acc = model.evaluate(X, y, verbose=0)\n",
        "\n",
        "print('Accuracy: %f' % acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8xFNOHXnj-9",
        "colab_type": "text"
      },
      "source": [
        "Using these two input features, we obtain an accuracy above 75%, which means that 3 out of 4 cars are classified correctly. We next plot the data points in the weight-horsepower space. Cars from the U.S. are shown in blue; cars from outside the U.S. are shown in red. The black line shows the decision boundary if we choose $\\tau = 0.5$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJh5PG61nj-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# determine intercept and slope of straight line corresponding to decision boundary\n",
        "weights = model.get_weights()\n",
        "intercept = (0.5 - weights[1][0]) / weights[0][1]\n",
        "slope = (weights[0][0] / weights[0][1])\n",
        "\n",
        "# plot data points and decision boundary\n",
        "cols = ['blue' if o==1 else 'red' for o in y]\n",
        "plt.scatter(cars_n['weight'].values, cars_n['horsepower'].values, color=cols)\n",
        "plt.plot(cars_n['weight'], intercept - slope*cars_n['weight'], color='black')\n",
        "plt.xlabel('LBS (normalized)')\n",
        "plt.ylabel('HP (normalized)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqTZ5iMznj_A",
        "colab_type": "text"
      },
      "source": [
        "In this second example, we again want to determine the origin of cars using Logistic Regression. More precisely, we only want to distinguish between cars from the U.S. and cars that are not from the U.S. As a first step, we thus need to derive suitable target values from the origin feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLUXUQWknj_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we are going to reuse our data matrix from the previous multiple linear regression example\n",
        "X = cars_n[['horsepower', 'weight', 'acceleration', 'mpg']].values\n",
        "\n",
        "# replace origin for Japanese and European cars by 0\n",
        "y = cars['origin'].replace([1,2,3], [1,0,0]).values\n",
        "\n",
        "# set up a model with a single neuron using a sigmoid activation function and binary cross entropy as loss function\n",
        "model = Sequential()\n",
        "model.add(Dense(1, activation='sigmoid', input_dim=X.shape[1]))\n",
        "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=30, batch_size=1, verbose=0)\n",
        "loss, acc = model.evaluate(X, y, verbose=0)\n",
        "\n",
        "print('Accuracy: %f' % acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDkHXy-fnj_D",
        "colab_type": "text"
      },
      "source": [
        "Here, we end up with an accuracy above 77%, which means that more than 3 out of 4 cars have been classified correctly. Taking into account two additional features helped to improve our classification accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0PxEP2rnj_E",
        "colab_type": "text"
      },
      "source": [
        "## 2.5 Gradient Descent\n",
        "\n",
        "In the following, we look at a proof-of-concept implementation of Gradient Descent. We revisit the problem of predicting (normalized) mpg based on (normalized) horsepower and use the sum of squared errors (SSE) as a loss function. The initial values for the two parameters $w_0$ and $w_1$ are determined randomly to lie in $[100,200]$. What you can observe when running the code is that the parameter choice of Gradient Descent moves toward the minimum, which is easy in this case, since our loss function is convex. Also observe that the components of the gradient computed become closer and closer to zero, while the value of the loss function decreases over time.\n",
        "\n",
        "By playing the values of the learning rate and the number of epochs, you can observe some typical problems of standard Gradient Descent. For instance, if we choose a learning rate that is too large, the computation quickly suffers from numerical issues (e.g., overflows of the gradient)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItZu_8HBnj_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "# sum of squared errors\n",
        "def sse(x, y, w0, w1):\n",
        "    return sum((y[i] - (w0 + w1*x[i]))**2 for i in range(0, len(y)))\n",
        "\n",
        "# gradient of sum of squared errors\n",
        "def sse_gradient(x, y, w0, w1):\n",
        "    w0d = -2*sum((y[i] - w0 - w1*x[i]) for i in range(0, len(y)))\n",
        "    w1d = -2*sum((y[i] - w0 - w1*x[i])*x[i] for i in range(0, len(y)))\n",
        "    return (w0d, w1d)\n",
        "\n",
        "# read the car data\n",
        "y = cars_n['mpg'].values\n",
        "x = cars_n['horsepower'].values\n",
        "\n",
        "# prepare data for contour plot\n",
        "W0 = np.arange(-200, 201, 10)\n",
        "W1 = np.arange(-200, 201, 10)\n",
        "n = len(W0)\n",
        "m = len(W1)\n",
        "Z = np.matrix(np.empty([n,m]))\n",
        "for i in range(0, n):\n",
        "    for j in range(0, m):\n",
        "        Z[i,j] = sse(x, y, W0[i], W1[j])\n",
        "\n",
        "# determine random initial parameters in [100,200]\n",
        "w0c, w1c = random.randint(100,201), random.randint(100,201)\n",
        "\n",
        "# number of epochs\n",
        "n_epochs = 1000\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# run gradient descent\n",
        "for epoch in range(1, n_epochs):\n",
        "    # plot contour\n",
        "    plt.contourf(W1, W0, Z, 50, cmap=plt.cm.coolwarm, vmax=Z.max(), vmin=Z.min())\n",
        "    plt.xlabel('$w_1$')\n",
        "    plt.ylabel('$w_0$')\n",
        "    plt.colorbar()  \n",
        "    \n",
        "    # print/plot current parameters\n",
        "    print(\"Current parameters: (%f, %f)\" % (w0c, w1c))\n",
        "    plt.scatter(w0c, w1c, color='black')\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "    # compute gradient\n",
        "    (w0u, w1u) = sse_gradient(x, y, w0c, w1c)\n",
        "    print(\"Gradient: (%f, %f)\" % (w0u, w1u))\n",
        "    print(\"Loss: %f\" % sse(x, y, w0c, w1c))\n",
        "    \n",
        "    # update current parameters\n",
        "    w0c = w0c - learning_rate*w0u\n",
        "    w1c = w1c - learning_rate*w1u"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lKzEVOQnj_H",
        "colab_type": "text"
      },
      "source": [
        "## 2.6 Evaluation\n",
        "\n",
        "Next, we look at how the concepts covered on the slides can be implemented using Pandas and Keras.\n",
        "\n",
        "### Splitting the Data into Training, Validation, and Test Data\n",
        "\n",
        "We will first make use of Pandas to split our data into training, validation, and test data. Note that there are other ways to accomplish this, for instance, by using sklean or numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH_G6rj1nj_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# percentages used for training, validation, and test data\n",
        "train_perc = 0.60\n",
        "val_perc = 0.20\n",
        "test_perc = 0.20\n",
        "\n",
        "# determine random permutation of DataFrame indexes\n",
        "perm = np.random.permutation(cars_hwam.index)\n",
        "\n",
        "# determine end indexes of training and validation data\n",
        "train_end = int(train_perc*len(perm))\n",
        "val_end = train_end + int(val_perc*len(perm))\n",
        "\n",
        "# extract training, validation, and test data\n",
        "cars_train = cars_hwam.iloc[perm[:train_end]]\n",
        "cars_val = cars_hwam.iloc[perm[train_end:val_end]]\n",
        "cars_test = cars_hwam.iloc[perm[val_end:]]\n",
        "\n",
        "print(\"Cars in training/validation/test data are %d/%d/%d\" % (len(cars_train), len(cars_val), len(cars_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw6x7_h8nj_J",
        "colab_type": "text"
      },
      "source": [
        "An alternative way to implement this is to use sklearn as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XG7z5XSnj_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# extract training data and temporary validation-test data\n",
        "cars_train, cars_tmp = train_test_split(cars_hwam, test_size=val_perc+test_perc)\n",
        "\n",
        "# split validation-test data into validation and test data\n",
        "cars_val, cars_test = train_test_split(cars_tmp, test_size=test_perc/(val_perc+test_perc))\n",
        "\n",
        "print(\"Cars in training/validation/test data are %d/%d/%d\" % (len(cars_train), len(cars_val), len(cars_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMKeUlQqnj_M",
        "colab_type": "text"
      },
      "source": [
        "### k-Fold Cross Validation\n",
        "\n",
        "While k-Fold Cross Validation can be implemented in a similar manner, it is easiest to make use of the functionality  built into sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjV0UAuynj_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# initialize 5-fold cross validation\n",
        "cv = KFold(n_splits=5, shuffle=True)\n",
        "for train_index, test_index in cv.split(cars_hwam):\n",
        "    print(\"Number of training data points: %d\" % len(train_index))\n",
        "    print(\"Number of test data points: %d\" % len(test_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7FuiZosnj_P",
        "colab_type": "text"
      },
      "source": [
        "### Quality Measures for Regression\n",
        "\n",
        "Next, we look into how the different quality measures for regression can be computed. To this end, we first train another linear regression model on our car dataset. Note that, when normalizing the data, we always use the minimum and maximum determined on the training data to avoid polluting our validation and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS_pBu42nj_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# normalize training, validation, and test data for car dataset\n",
        "cars_train_n = (cars_train - cars_train.min()) / (cars_train.max() - cars_train.min())\n",
        "cars_val_n = (cars_val - cars_train.min()) / (cars_train.max() - cars_train.min())\n",
        "cars_test_n = (cars_test - cars_train.min()) / (cars_train.max() - cars_train.min())\n",
        "\n",
        "# prepare training, validation, and test data\n",
        "X_train = cars_train_n[['horsepower', 'weight', 'acceleration']].values\n",
        "y_train = cars_train_n['mpg']\n",
        "X_val = cars_val_n[['horsepower', 'weight', 'acceleration']].values\n",
        "y_val = cars_val_n['mpg']\n",
        "X_test = cars_test_n[['horsepower', 'weight', 'acceleration']].values\n",
        "y_test = cars_test_n['mpg']\n",
        "\n",
        "# set up model with a single neuron using a linear activation function and MSE as loss function\n",
        "model = Sequential()\n",
        "model.add(Dense(1, activation='linear', input_dim=X_train.shape[1]))\n",
        "model.compile(optimizer='SGD', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=1, verbose=0, validation_data=(X_val, y_val))\n",
        "\n",
        "# make predictions for test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# compute prediction quality measures\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error: %f\" % mse)\n",
        "print(\"Root Mean Squared Error: %f\" % math.sqrt(mse))\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean Absolute Error: %f\" % mae)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R2 Coefficient of Determination: %f\" % r2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBh5tK9Snj_R",
        "colab_type": "text"
      },
      "source": [
        "### Quality Measures for Classification\n",
        "\n",
        "In the following, we use logistic regression to classify our cars according to their origin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VK6Xcxxnj_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# extract training data and temporary validation-test data\n",
        "cars_train, cars_tmp = train_test_split(cars, test_size=val_perc+test_perc)\n",
        "\n",
        "# split validation-test data into validation and test data\n",
        "cars_val, cars_test = train_test_split(cars_tmp, test_size=test_perc/(val_perc+test_perc))\n",
        "\n",
        "# normalize data -- note that origin can not be normalized\n",
        "X_train = cars_train[['horsepower', 'weight', 'acceleration']].values\n",
        "X_train_n = (X_train - X_train.min(axis=0)) / (X_train.max(axis=0) - X_train.min(axis=0))\n",
        "X_val = cars_val[['horsepower', 'weight', 'acceleration']].values\n",
        "X_val_n = (X_val - X_train.min(axis=0)) / (X_train.max(axis=0) - X_train.min(axis=0))\n",
        "X_test = cars_test[['horsepower', 'weight', 'acceleration']].values\n",
        "X_test_n = (X_test - X_train.min(axis=0)) / (X_train.max(axis=0) - X_train.min(axis=0))\n",
        "\n",
        "# make origin binary\n",
        "y_train = cars_train['origin'].replace([1,2,3], [1,0,0]).values\n",
        "y_val = cars_val['origin'].replace([1,2,3], [1,0,0]).values\n",
        "y_test = cars_test['origin'].replace([1,2,3], [1,0,0]).values\n",
        "\n",
        "# train a logistic regression model\n",
        "model = Sequential()\n",
        "model.add(Dense(1, activation='sigmoid', input_dim=X_train.shape[1]))\n",
        "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_n, y_train, epochs=30, batch_size=1, verbose=0, validation_data=(X_val_n, y_val))\n",
        "\n",
        "# compute predictions by thresholding\n",
        "threshold = 0.5\n",
        "y_pred = model.predict(X_test_n) >= threshold\n",
        "\n",
        "# compute confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print()\n",
        "\n",
        "# compute Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: %f\" % acc)\n",
        "\n",
        "# compute Precision\n",
        "from sklearn.metrics import precision_score\n",
        "prec = precision_score(y_test, y_pred)\n",
        "print(\"Precision: %f\" % prec)\n",
        "\n",
        "# compute Recall\n",
        "from sklearn.metrics import recall_score\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print(\"Recall: %f\" % recall)\n",
        "\n",
        "# compute F1\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(\"F1: %f\" % f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmHb592pnj_T",
        "colab_type": "text"
      },
      "source": [
        "## 2.7 Overfitting\n",
        "\n",
        "We will look into how to detect and counter overfitting when working with other datasets."
      ]
    }
  ]
}